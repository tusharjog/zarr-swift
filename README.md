# Zarr v3 Swift Library

A comprehensive, production-ready Swift implementation of the [Zarr v3 specification](https://zarr-specs.readthedocs.io/) for reading and writing chunked, compressed N-dimensional arrays. Designed for scientific computing, machine learning, and large-scale data processing on macOS and Linux.

[![Platform](https://img.shields.io/badge/platform-macOS%20%7C%20Linux-lightgrey)]()
[![Swift](https://img.shields.io/badge/Swift-5.0+-orange)]()
[![License](https://img.shields.io/badge/license-MIT-blue)]()

## Overview

Zarr is a format for storing chunked, compressed N-dimensional arrays, optimized for cloud storage and parallel computing. This Swift library provides a complete implementation of the Zarr v3 specification with:

- ‚úÖ **Full Zarr v3 compliance** - Groups, arrays, metadata, and extensions
- ‚úÖ **Cloud-native design** - Built for S3, GCS, Azure, and HTTP access
- ‚úÖ **Multiple storage backends** - Filesystem, memory, ZIP, and HTTP
- ‚úÖ **Flexible codec pipeline** - Compression, checksums, and custom transforms
- ‚úÖ **Type-safe API** - No runtime type casting, leveraging Swift's type system
- ‚úÖ **Cross-platform** - Works on macOS and Linux

## Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Architecture](#architecture)
- [Core Concepts](#core-concepts)
- [API Reference](#api-reference)
- [Storage Backends](#storage-backends)
- [Codecs & Compression](#codecs--compression)
- [Working with Hierarchies](#working-with-hierarchies)
- [Performance Considerations](#performance-considerations)
- [Zarr v3 Specification Coverage](#zarr-v3-specification-coverage)
- [Comparison with Other Formats](#comparison-with-other-formats)
- [Future Improvements](#future-improvements)
- [Contributing](#contributing)
- [License](#license)

## Features

### Core Functionality

- **Hierarchical Data Organization**
  - Groups for organizing arrays and sub-groups
  - Full hierarchy traversal and search utilities
  - Path-based node resolution

- **N-Dimensional Arrays**
  - Support for 1D to N-dimensional arrays
  - Chunked storage for efficient I/O
  - Named dimensions for semantic clarity

- **Rich Data Types**
  - Integers: `int8`, `int16`, `int32`, `int64`, `uint8`, `uint16`, `uint32`, `uint64`
  - Floats: `float32`, `float64`
  - Complex: `complex64`, `complex128`
  - Strings: variable-length and fixed-length
  - Raw/opaque types: `r8`, `r16`, etc.

- **Codec Pipeline**
  - `bytes` - Endianness conversion
  - `gzip` - Standard compression (levels 0-9)
  - `zstd` - Fast compression (levels 1-22)
  - `transpose` - Dimension reordering
  - `crc32c` - Data integrity checksums
  - Extensible codec system

- **Storage Backends**
  - **Filesystem** - Local file storage
  - **Memory** - In-memory storage for testing
  - **ZIP** - Read-only ZIP archive support
  - **HTTP** - Remote read-only access with caching

- **Type-Safe Attributes**
  - `AttributeValue` enum for metadata (no `Any` types)
  - Literal syntax support for clean code
  - Pattern matching and safe access

- **Node Protocol**
  - Unified `ZarrNode` protocol for polymorphic operations
  - Mixed collections of arrays and groups
  - Generic hierarchy traversal

### Advanced Features

- **Fill Values** - Default values for uninitialized chunks
- **Dimension Names** - Semantic labels for array dimensions
- **Chunk Key Encoding** - Default and v2-compatible encodings
- **Custom Attributes** - Arbitrary metadata on groups and arrays
- **Tree Visualization** - Print hierarchical structure
- **Attribute Search** - Find nodes by attribute keys
- **Thread Safety** - All store operations are thread-safe

## Installation

### Swift Package Manager

Add to your `Package.swift`:

```swift
dependencies: [
    .package(url: "https://github.com/yourusername/zarr-swift.git", from: "1.0.0")
]
```

### Manual Integration

Copy the entire library source file into your project. The library is self-contained with no external dependencies beyond Foundation.

## Quick Start

### Creating a Simple Array

```swift
import Foundation

// Create a filesystem store
let store = try FilesystemStore(path: URL(fileURLWithPath: "/tmp/my_data.zarr"))

// Create a 2D array
let array = try ZarrArray.create(
    store: store,
    path: "temperature",
    shape: [365, 100],           // 365 days √ó 100 locations
    chunkShape: [10, 10],        // 10√ó10 chunks
    dataType: .float32,
    fillValue: .float(0.0),
    dimensionNames: ["time", "location"],
    attributes: [
        "units": "celsius",
        "description": "Daily temperature measurements"
    ]
)

// Write a chunk of data
let temps: [Float] = Array(repeating: 23.5, count: 100)
try array.writeChunk(at: [0, 0], data: temps)

// Read it back
let readTemps: [Float] = try array.readChunk(at: [0, 0], as: Float.self)
print("Temperature: \(readTemps[0])¬∞C")
```

### Creating a Hierarchy

```swift
// Create root group
let root = try ZarrGroup.create(
    store: store,
    path: "",
    attributes: ["version": "1.0"]
)

// Create nested structure
let experiments = try root.createGroup(name: "experiments")
let results = try experiments.createGroup(name: "results")

// Add arrays at any level
let pressure = try experiments.createArray(
    name: "pressure",
    shape: [1000, 1000],
    chunkShape: [100, 100],
    dataType: .float64,
    codecs: [.bytes(endian: "little"), .gzip(level: 5)]
)

// Write compressed data
let data = Array(repeating: 101.325, count: 10000)
try pressure.writeChunk(at: [0, 0], data: data)
```

### Working with Remote Data

```swift
// Access data over HTTP
let httpStore = HTTPStore(
    baseURL: URL(string: "https://example.com/climate-data.zarr")!,
    cacheSize: 100
)

let remoteGroup = try ZarrGroup.open(store: httpStore, path: "")
let remoteArray = try remoteGroup.getArray(name: "temperature")

// Read chunks as needed
let chunk: [Float] = try remoteArray.readChunk(at: [0, 0, 0], as: Float.self)
```

## Architecture

### Design Principles

1. **Value Semantics for Data** - Metadata structs are immutable value types
2. **Reference Semantics for I/O** - Groups and arrays are classes managing resources
3. **Protocol-Oriented** - Extensible through protocols (`ZarrStore`, `ZarrNode`)
4. **Type Safety** - No `Any` types; explicit enums for heterogeneous data
5. **Thread Safety** - All stores use locks for concurrent access
6. **Zero Dependencies** - Only uses Foundation framework

### Class Hierarchy

```
ZarrNode (protocol)
    ‚îú‚îÄ‚îÄ ZarrGroup (class)
    ‚îî‚îÄ‚îÄ ZarrArray (class)

ZarrStore (protocol)
    ‚îú‚îÄ‚îÄ FilesystemStore (class)
    ‚îú‚îÄ‚îÄ MemoryStore (class)
    ‚îú‚îÄ‚îÄ ZipStore (class)
    ‚îî‚îÄ‚îÄ HTTPStore (class)

Metadata (structs)
    ‚îú‚îÄ‚îÄ ZarrGroupMetadata
    ‚îú‚îÄ‚îÄ ZarrArrayMetadata
    ‚îú‚îÄ‚îÄ ChunkGrid
    ‚îú‚îÄ‚îÄ ChunkKeyEncoding
    ‚îî‚îÄ‚îÄ Codec
```

### Why Classes for Groups/Arrays?

Groups and arrays are **I/O abstractions** that manage:
- File handles
- Network connections
- Caches
- Locks

These resources need **reference semantics** - copying would create confusion about identity and resource ownership. This follows the same pattern as Foundation's `FileHandle`, `URLSession`, etc.

### Why Structs for Metadata?

Metadata is **pure data** with no I/O:
- Safe to copy
- Immutable after creation
- No identity concerns
- Efficient value semantics

## Core Concepts

### Chunking

Arrays are divided into equal-sized chunks for efficient I/O:

```
Array: [10000, 10000]
Chunks: [1000, 1000]
Result: 100 chunks (10√ó10 grid)

Each chunk is stored as a separate object:
  c/0/0, c/0/1, ..., c/9/9
```

**Benefits:**
- Parallel reads/writes (different chunks)
- Partial loading (only read what you need)
- Independent compression (per chunk)
- Cloud-optimized (each chunk = one object)

### Codec Pipeline

Codecs transform data before storage:

```
Data ‚Üí Transpose ‚Üí Bytes ‚Üí Gzip ‚Üí CRC32C ‚Üí Storage
```

Example:
```swift
let codecs: [Codec] = [
    .transpose(order: [2, 1, 0]),  // Reorder dimensions
    .bytes(endian: "little"),       // Convert to bytes
    .gzip(level: 6),                // Compress
    .crc32c                         // Add checksum
]
```

### Metadata Structure

Every group and array has a `zarr.json` file:

**Group:**
```json
{
  "zarr_format": 3,
  "node_type": "group",
  "attributes": {
    "description": "Experimental data"
  }
}
```

**Array:**
```json
{
  "zarr_format": 3,
  "node_type": "array",
  "shape": [1000, 1000],
  "data_type": "float64",
  "chunk_grid": {
    "name": "regular",
    "configuration": {"chunk_shape": [100, 100]}
  },
  "codecs": [
    {"name": "bytes", "configuration": {"endian": "little"}},
    {"name": "gzip", "configuration": {"level": 5}}
  ],
  "dimension_names": ["x", "y"]
}
```

## API Reference

### ZarrGroup

```swift
// Create a new group
static func create(
    store: ZarrStore,
    path: String = "",
    attributes: [String: AttributeValue]? = nil
) throws -> ZarrGroup

// Open existing group
static func open(store: ZarrStore, path: String = "") throws -> ZarrGroup

// Create child array
func createArray(
    name: String,
    shape: [Int],
    chunkShape: [Int],
    dataType: ZarrDataType,
    fillValue: FillValue = .null,
    codecs: [Codec] = [.bytes(endian: "little")],
    attributes: [String: AttributeValue]? = nil,
    dimensionNames: [String]? = nil
) throws -> ZarrArray

// Create child group
func createGroup(name: String, attributes: [String: AttributeValue]? = nil) throws -> ZarrGroup

// Access children
func getArray(name: String) throws -> ZarrArray
func getGroup(name: String) throws -> ZarrGroup
func getNode(name: String) throws -> ZarrNode?

// List and search
func listChildren() throws -> [String]
func listNodes() throws -> [ZarrNode]
func collectAllNodes() throws -> [ZarrNode]
func findNodes(withAttribute key: String) throws -> [ZarrNode]

// Path resolution
func resolveNode(path: String) throws -> ZarrNode?

// Management
func hasChild(name: String) -> Bool
func deleteChild(name: String) throws

// Tree visualization
func buildTree() throws -> ZarrTreeItem
```

### ZarrArray

```swift
// Create a new array
static func create(
    store: ZarrStore,
    path: String = "",
    shape: [Int],
    chunkShape: [Int],
    dataType: ZarrDataType,
    fillValue: FillValue = .null,
    codecs: [Codec] = [.bytes(endian: "little")],
    attributes: [String: AttributeValue]? = nil,
    dimensionNames: [String]? = nil
) throws -> ZarrArray

// Open existing array
static func open(store: ZarrStore, path: String = "") throws -> ZarrArray

// Write data
func writeChunk<T: BinaryInteger>(at indices: [Int], data: [T]) throws
func writeChunk<T: BinaryFloatingPoint>(at indices: [Int], data: [T]) throws
func writeChunk(at indices: [Int], strings: [String]) throws

// Read data
func readChunk<T: BinaryInteger>(at indices: [Int], as type: T.Type) throws -> [T]
func readChunk<T: BinaryFloatingPoint>(at indices: [Int], as type: T.Type) throws -> [T]
func readChunk(at indices: [Int], asStrings: Bool = true) throws -> [String]
```

### ZarrNode Protocol

```swift
protocol ZarrNode: AnyObject {
    var path: String { get }
    var store: ZarrStore { get }
    var attributes: [String: AttributeValue]? { get }
    var name: String { get }
    
    func isGroup() -> Bool
    func isArray() -> Bool
    func asGroup() -> ZarrGroup?
    func asArray() -> ZarrArray?
}
```

### AttributeValue

```swift
enum AttributeValue {
    case null
    case bool(Bool)
    case int(Int)
    case double(Double)
    case string(String)
    case array([AttributeValue])
    case dictionary([String: AttributeValue])
}

// Supports Swift literals
let attrs: [String: AttributeValue] = [
    "version": "1.0",        // String literal
    "count": 42,             // Int literal
    "enabled": true,         // Bool literal
    "temperature": 23.5,     // Double literal
    "tags": ["a", "b", "c"]  // Array literal
]

// Safe access
if let version = attrs["version"]?.stringValue {
    print(version)
}
```

## Storage Backends

### FilesystemStore

Local filesystem storage:

```swift
let store = try FilesystemStore(path: URL(fileURLWithPath: "/data/zarr"))
```

**Best for:**
- Local processing
- Development/testing
- Single-machine workflows

### MemoryStore

In-memory storage:

```swift
let store = MemoryStore()
```

**Best for:**
- Unit tests
- Temporary data
- Fast prototyping

### ZipStore (Read-Only)

Read Zarr data from ZIP archives:

```swift
let store = try ZipStore(
    path: URL(fileURLWithPath: "/data/archive.zarr.zip"),
    mode: .read
)
```

**Features:**
- Parses ZIP central directory
- Supports DEFLATE and STORED compression
- Thread-safe access

**Best for:**
- Distributing datasets
- Archival storage
- Single-file deployment

### HTTPStore (Read-Only)

Access remote Zarr data over HTTP/HTTPS:

```swift
let store = HTTPStore(
    baseURL: URL(string: "https://example.com/data.zarr")!,
    cacheSize: 100
)
```

**Features:**
- Automatic caching (NSCache)
- HEAD request support
- Synchronous API with URLSession

**Best for:**
- Public datasets
- Cloud-hosted static files
- Remote analysis

## Codecs & Compression

### Available Codecs

| Codec | Type | Purpose | Configuration |
|-------|------|---------|--------------|
| `bytes` | Array‚ÜíBytes | Endianness | `endian: "little"` or `"big"` |
| `transpose` | Array‚ÜíArray | Reorder dims | `order: [2, 1, 0]` |
| `gzip` | Bytes‚ÜíBytes | Compression | `level: 0-9` |
| `zstd` | Bytes‚ÜíBytes | Fast compression | `level: 1-22` |
| `crc32c` | Bytes‚ÜíBytes | Checksum | None |

### Creating a Pipeline

```swift
let codecs: [Codec] = [
    .bytes(endian: "little"),  // Must have array‚Üíbytes codec
    .gzip(level: 6),           // Optional compression
    .crc32c                    // Optional checksum
]

let array = try ZarrArray.create(
    store: store,
    path: "data",
    shape: [1000, 1000],
    chunkShape: [100, 100],
    dataType: .float64,
    codecs: codecs
)
```

### Compression Guidelines

| Use Case | Codec | Level | Notes |
|----------|-------|-------|-------|
| **Maximum compression** | `gzip` | 9 | Slow, best ratio |
| **Balanced** | `gzip` or `zstd` | 5-6 | Good compromise |
| **Speed priority** | `zstd` | 1-3 | Fast, decent compression |
| **No compression** | `bytes` only | N/A | Fastest, no savings |

## Working with Hierarchies

### Traversal

```swift
let root = try ZarrGroup.open(store: store, path: "")

// List direct children
let children = try root.listChildren()

// Get all nodes recursively
let allNodes = try root.collectAllNodes()

// Count by type
let arrayCount = allNodes.filter { $0.isArray() }.count
let groupCount = allNodes.filter { $0.isGroup() }.count
```

### Search

```swift
// Find nodes with specific attribute
let withUnits = try root.findNodes(withAttribute: "units")

// Path-based access
if let node = try root.resolveNode(path: "experiments/results/temperature") {
    if let array = node.asArray() {
        let data = try array.readChunk(at: [0, 0], as: Float.self)
    }
}
```

### Visualization

```swift
let tree = try root.buildTree()
tree.printTree()

// Output:
// üìÅ /
//   üìÅ experiments/
//     üìä temperature [365√ó180√ó360] float32
//        attributes: units, description
//     üìÅ results/
//       üìä pressure [1000√ó1000] float64
```

### Mixed Collections

```swift
// Store arrays and groups together
let nodes: [ZarrNode] = try root.collectAllNodes()

// Filter to arrays only
let arrays = nodes.compactMap { $0.asArray() }

// Get specific data type
let float64Arrays = nodes.compactMap { node -> ZarrArray? in
    guard let array = node.asArray() else { return nil }
    return array.metadata.dataType == .float64 ? array : nil
}
```

## Performance Considerations

### Chunk Size Guidelines

| Array Size | Good Chunk Size | Reasoning |
|-----------|----------------|-----------|
| 1000√ó1000 | 100√ó100 | ~1-10 MB per chunk |
| 10000√ó10000 | 1000√ó1000 | Balance overhead vs size |
| 100√ó100√ó100 | 10√ó10√ó10 | 3D: smaller chunks |

**Rules of thumb:**
- Target 1-10 MB per chunk (uncompressed)
- Align chunks with access patterns
- Smaller chunks = more overhead but better parallelism
- Larger chunks = less overhead but coarser granularity

### Local vs Cloud

**Local Filesystem:**
- Larger chunks are fine (less overhead)
- HDF5 may be faster for single-threaded
- Zarr excels in parallel workloads

**Cloud Storage:**
- Smaller chunks for better parallelism
- Each chunk = separate HTTP request
- Use sharding (future) to reduce object count

### Caching Strategies

**HTTPStore:**
```swift
let store = HTTPStore(
    baseURL: url,
    cacheSize: 100  // Cache last 100 chunks
)
```

**Custom cache:**
```swift
class NodeCache {
    private var cache: [String: ZarrNode] = [:]
    
    func preload(_ paths: [String], from root: ZarrGroup) throws {
        for path in paths {
            if let node = try root.resolveNode(path: path) {
                cache[path] = node
            }
        }
    }
}
```

## Zarr v3 Specification Coverage

### Implemented Features (~60%)

- ‚úÖ Core metadata (groups, arrays)
- ‚úÖ Hierarchical organization
- ‚úÖ All basic data types (int, uint, float, complex, string, raw)
- ‚úÖ Regular chunk grids
- ‚úÖ Chunk key encoding (default, v2)
- ‚úÖ Fill values
- ‚úÖ Dimension names
- ‚úÖ Codec pipeline
- ‚úÖ Core codecs (bytes, gzip, zstd, transpose, crc32c)
- ‚úÖ Custom attributes (type-safe)
- ‚úÖ Multiple storage backends
- ‚úÖ Thread-safe operations

### Missing Features (~40%)

- ‚ùå Blosc codec (important for scientific data)
- ‚ùå Sharding extension (critical for cloud at scale)
- ‚ùå Variable-length types (vlen-utf8, vlen-bytes)
- ‚ùå Structured/compound types
- ‚ùå Alternative chunk grids
- ‚ùå Zarr v2 compatibility (read/write)
- ‚ùå Array slicing (read arbitrary regions, not just chunks)
- ‚ùå Async I/O
- ‚ùå S3/GCS/Azure native stores
- ‚ùå Consolidated metadata
- ‚ùå `must_understand` extension field

## Comparison with Other Formats

| Feature | Zarr v3 | NetCDF-4 | HDF5 | N5 |
|---------|---------|----------|------|-----|
| **Cloud Performance** | ‚úÖ Excellent | ‚ùå Poor | ‚ùå Poor | ‚úÖ Excellent |
| **Parallel Writes** | ‚úÖ Yes | ‚ùå Limited | ‚ö†Ô∏è MPI only | ‚úÖ Yes |
| **File Structure** | Directory | Single file | Single file | Directory |
| **Metadata** | JSON | Binary | Binary | JSON |
| **Maturity** | Moderate | Very high | Very high | Moderate |
| **Ecosystem** | Growing | Very large | Very large | Smaller |

**Choose Zarr when:**
- Working with cloud storage
- Need parallel writes
- Building modern data pipelines
- Flexibility is important

**Choose NetCDF when:**
- Climate/atmospheric science
- CF conventions required
- Legacy compatibility needed

**Choose HDF5 when:**
- Maximum local performance
- Complex hierarchies
- Broad tool support needed

## Future Improvements

### High Priority

1. **Blosc Codec**
   - Most common compressor in scientific computing
   - Multiple internal algorithms (lz4, zstd, blosclz)
   - Shuffle/bitshuffle support
   - **Impact**: Essential for scientific data compatibility

2. **Sharding Extension (ZEP 2)**
   - Combine multiple chunks into single objects
   - Reduces object count for cloud storage
   - Critical for production cloud deployments
   - **Impact**: 10-100√ó fewer objects = better performance

3. **Array Slicing**
   - Read arbitrary regions, not just full chunks
   - `array[0:100, 50:150]` syntax
   - More convenient API
   - **Impact**: Much easier to use

4. **Cloud Store Backends**
   - Native S3 support (AWS SDK)
   - Native GCS support (Google Cloud SDK)
   - Native Azure Blob support
   - **Impact**: Production-ready cloud deployments

### Medium Priority

5. **Async/Await API**
   - Modern Swift concurrency
   - Non-blocking I/O
   - Better performance for network stores
   - **Impact**: Cleaner async code

6. **Zarr v2 Compatibility**
   - Read v2 format
   - Write v2 format
   - Migration tools (v2 ‚Üí v3)
   - **Impact**: Interop with existing datasets

7. **Variable-Length Types**
   - `vlen-utf8` codec
   - `vlen-bytes` codec
   - Efficient string storage
   - **Impact**: Better string array performance

8. **Consolidated Metadata**
   - Single JSON for entire hierarchy
   - Faster cloud metadata access
   - Reduced latency
   - **Impact**: Significant speedup for cloud

### Low Priority

9. **Structured Types**
   - Compound data structures
   - Record arrays
   - **Impact**: Niche use cases

10. **BigEndian Support**
    - Full big-endian handling
    - Platform-agnostic
    - **Impact**: Rare platforms

11. **Advanced Extensions**
    - `must_understand` field
    - URI-based extension names
    - Extension registration
    - **Impact**: Full spec compliance

12. **Performance Optimizations**
    - Batch chunk operations
    - Prefetching
    - Parallel codec processing
    - Memory pooling
    - **Impact**: Faster for large operations

13. **Developer Tools**
    - CLI for inspection
    - Validation tools
    - Migration utilities
    - Benchmarking suite
    - **Impact**: Better DX

## Contributing

Contributions are welcome! Areas where help is needed:

1. **Blosc codec implementation**
2. **Sharding extension**
3. **Cloud storage backends (S3, GCS, Azure)**
4. **Array slicing API**
5. **Documentation improvements**
6. **Test coverage expansion**
7. **Performance benchmarks**

### Development Setup

```bash
git clone https://github.com/yourusername/zarr-swift.git
cd zarr-swift
swift build
swift test
```

### Running Tests

```bash
# Run all tests
swift test

# Run specific test
swift test --filter ZarrArrayTests
```

## License

MIT License - see LICENSE file for details.

## References

- [Zarr v3 Specification](https://zarr-specs.readthedocs.io/)
- [Zarr Python Implementation](https://github.com/zarr-developers/zarr-python)
- [N5 Format](https://github.com/saalfeldlab/n5)
- [NetCDF](https://www.unidata.ucar.edu/software/netcdf/)
- [HDF5](https://www.hdfgroup.org/solutions/hdf5/)

## Acknowledgments

This library implements the Zarr v3 specification developed by the Zarr community. Special thanks to the Zarr developers and the scientific computing community for creating and maintaining this excellent format.

---

**Questions?** Open an issue on GitHub or reach out to the maintainers.

**Want to learn more about Zarr?** Check out the [official documentation](https://zarr.readthedocs.io/).




## License

MIT License - see LICENSE file for details.

## References

- [Zarr v3 Specification](https://zarr-specs.readthedocs.io/)
- [Zarr Python Implementation](https://github.com/zarr-developers/zarr-python)
- [N5 Format](https://github.com/saalfeldlab/n5)
- [NetCDF](https://www.unidata.ucar.edu/software/netcdf/)
- [HDF5](https://www.hdfgroup.org/solutions/hdf5/)

## Acknowledgments

This library implements the Zarr v3 specification developed by the Zarr community. Special thanks to the Zarr developers and the scientific computing community for creating and maintaining this excellent format.

---

**Questions?** Open an issue on GitHub or reach out to the maintainers.

**Want to learn more about Zarr?** Check out the [official documentation](https://zarr.readthedocs.io/).
